# LLM4EntityMatching
This is the code repository for our work related to the general entity matching (EM) task and the Asset Administration Shell (AAS)-specific EM task.
## Selective Entity Matching
In the paper "Fine-Tuning Large Language Models with Contrastive Margin Ranking Loss for Selective Entity Matching in Product Data Integration" (submitted), we first revisit the standard pairwise EM setting by recompiling existing benchmark datasets to include more hard negative candidates, which are semantically similar to corresponding query entities. We then evaluate state-of-the-art (SOTA) pairwise matchers on these recompiled datasets, revealing the limitations of the conventional pairwise EM approach under more challenging and realistic conditions. Second, we propose a selective EM approach that formulates EM as a listwise selection task, where the query entity is compared directly with the entire candidate set rather than evaluated through independent pairwise classifications. Accordingly, a new evaluation framework is introduced, including recompiled benchmark datasets and a new evaluation metric. Third, we propose a selective EM method Mistral4SelectEM, which fine-tunes an LLM-based embedding model for selective EM by structuring it into a Siamese network and fine-tuning it with a novel contrastive margin ranking loss (CMRL). It aims to enhance the model’s ability to distinguish true positives from semantically similar negatives. 
### Method
![](/resource/Mistral4SelectEM.png)
*Fig. 3. Illustration of the end-to-end selective EM (left) and the fine-tuning strategy for Mistral4SelectEM (right). The entire process involves a single set of LLM weights, which is presented as the green block. The “red” adapter is the fine-tuned LoRA weights, which are merged with the embedding model in the inference stage for selective EM*<br>  
This method is implemented in /tasks/SelectiveEntityMatching/selection_llm. The comparative methods are implemented in selection_plm, selection_gpt, and cross_selection_llm. The selection of the methods is controlled by the main args.method.
### data
The original EM benchmark datasets, i.e., "Structured_Amazon-Google", "Structured_Walmart-Amazon", and "Textual_Abt-Buy" are recompiled. The recompilation process begins with the retention of labeled positive match pairs from the existing train, validation, and test sets. For each query entity in these positive pairs, we implement a blocking step by combining an embedding model with an efficient indexing and search algorithm. In this work, we adopt the SOTA LLM-based embedding model "Linq-Embed-Mistral" and utilize the FAISS library for indexing and searching. The embedding model encodes both query and candidate entities into vector representations, and the top-K candidates are retrieved and ranked based on semantic similarity. The choice of the embedding model and the selection of K=10 is informed by the ablation study in Section 4.4.1. If the retrieved candidates include entities already present in the original dataset, their labels are directly transferred to the recompiled dataset. For all other retrieved candidates, negative labels are assigned. It ensures that each query entity is associated with exactly K candidates. In most cases, there is a single true positive candidate; however, there are instances where multiple true positive candidates exist.  
The original data (fixed train-valid-test split) is still available as train_df.csv, valid_df.csv, test_df.csv. The recompiled dataset is structured in two formats. The first format maintains pairwise data, consistent with the original dataset, for use in pairwise EM methods, named as e.g. test_df_pairwise.csv. The second format adopts an information-retrieval-style representation, where the label for a query is a binary match vector, indicating the position of true positive candidates in the top-K retrieved set. They are named as e.g. test_df_new.csv.
